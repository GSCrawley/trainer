{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18135fc4",
   "metadata": {},
   "source": [
    "# Gretel Trainer\n",
    "\n",
    "This notebook is designed to help users successfully train synthetic models on complex datasets with high row and column counts. The code works by intelligently dividing a dataset into a set of smaller datasets of correlated columns that can be parallelized and then joined together. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68871bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import strategy\n",
    "import runner\n",
    "\n",
    "from gretel_client.projects import create_or_get_unique_project\n",
    "from gretel_client.projects.models import read_model_config\n",
    "from gretel_client.projects.jobs import Status\n",
    "from gretel_synthetics.utils.header_clusters import cluster\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3b0120",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parallelization parameters and options\n",
    "\n",
    "MAX_ROWS = 20000 # Maximum row count per model\n",
    "MAX_HEADER_CLUSTERS = 20 # Max columns per cluster\n",
    "GENERATE_ROWS = 0 # Use zero to match row count from training data\n",
    "CACHE_FILE = \"runner.json\"\n",
    "PROJECT = create_or_get_unique_project(name=\"complex-dataset\")\n",
    "\n",
    "print(f\"Follow model training at: {PROJECT.get_console_url()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc77f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset to synthesize\n",
    "\n",
    "DATASET_PATH = 'cpu_states.csv'\n",
    "ROUND_DECIMALS = 4\n",
    "\n",
    "\n",
    "def preprocess_data(dataset_path: str) -> pd.DataFrame:\n",
    "    tmp = pd.read_csv(dataset_path, low_memory=False)\n",
    "    tmp = tmp.round(ROUND_DECIMALS)\n",
    "    return tmp\n",
    "    \n",
    "DF = preprocess_data(DATASET_PATH)\n",
    "DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18926173",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a default configuration from GitHub\n",
    "\n",
    "CONFIG = read_model_config(\"synthetics/default\")\n",
    "CONFIG[\"models\"][0][\"synthetics\"][\"params\"][\"learning_rate\"] = 0.001\n",
    "CONFIG[\"models\"][0][\"synthetics\"][\"privacy_filters\"] = {}\n",
    "CONFIG[\"models\"][0][\"synthetics\"][\"privacy_filters\"][\"outliers\"] = None\n",
    "CONFIG[\"models\"][0][\"synthetics\"][\"privacy_filters\"][\"similarity\"] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645d2859",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the parallelization strategy\n",
    "\n",
    "def initialize_run() -> runner.StrategyRunner:\n",
    "    \n",
    "    # Create clusters of correlated columns (might take a few minutes)\n",
    "    header_clusters = cluster(DF, maxsize=MAX_HEADER_CLUSTERS, plot=True) \n",
    "\n",
    "    constraints = strategy.PartitionConstraints(\n",
    "        header_clusters=header_clusters, \n",
    "        max_row_count=MAX_ROWS\n",
    "    )\n",
    "    \n",
    "    run = runner.StrategyRunner(\n",
    "        strategy_id=\"foo\",\n",
    "        df=DF,\n",
    "        cache_file=CACHE_FILE,\n",
    "        cache_overwrite=True,  # False means we'll try and load an existing cache and start back up, otherwise start fresh\n",
    "        model_config=CONFIG,\n",
    "        partition_constraints=constraints,\n",
    "        project=PROJECT\n",
    "    )    \n",
    "    return run\n",
    "\n",
    "run = initialize_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e17f0099",
   "metadata": {},
   "outputs": [],
   "source": [
    "run.train_all_partitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e67bc9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "synthetic = run.get_training_synthetic_data()\n",
    "synthetic.to_csv('synthetic.csv', index=False)\n",
    "synthetic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1460ea07",
   "metadata": {},
   "outputs": [],
   "source": [
    "#run.cancel_all()\n",
    "#PROJECT.delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28cf5ee4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
