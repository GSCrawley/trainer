{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18135fc4",
   "metadata": {},
   "source": [
    "# Gretel Trainer\n",
    "\n",
    "This notebook is designed to help users successfully train synthetic models on complex datasets with high row and column counts. The code works by intelligently dividing a dataset into a set of smaller datasets of correlated columns that can be parallelized and then stitched back together. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68871bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import strategy\n",
    "import runner\n",
    "\n",
    "from gretel_client.projects import create_or_get_unique_project\n",
    "from gretel_client.projects.models import read_model_config\n",
    "from gretel_client.projects.jobs import Status\n",
    "from gretel_synthetics.utils.header_clusters import cluster\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3b0120",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_ROWS = 50000  # Maximum row count per model\n",
    "CLUSTER_SIZE = 20 # Max columns per cluster\n",
    "CACHE_FILE = \"runner.json\"\n",
    "\n",
    "PROJECT = create_or_get_unique_project(name=\"trainer-dataset\")\n",
    "print(f\"Follow model training at: {PROJECT.get_console_url()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc77f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset to synthesize\n",
    "DF = pd.read_csv(\"cpu_states.csv\", low_memory=False)\n",
    "DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9533a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use header clustering to create a strategy for parallel training\n",
    "# This may take a few minutes\n",
    "HEADER_CLUSTERS = cluster(DF)\n",
    "print(f\"Initial dataset size: {DF.shape[0]} rows {DF.shape[1]} columns\")\n",
    "print(f\"  --> Created {len(HEADER_CLUSTERS)} clusters for training with {len(HEADER_CLUSTERS[1])} columns each.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18926173",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a default configuration from GitHub\n",
    "CONFIG = read_model_config(\"synthetics/mostly-numeric-data\")\n",
    "\n",
    "CONFIG[\"models\"][0][\"synthetics\"][\"params\"][\"epochs\"] = 200\n",
    "CONFIG[\"models\"][0][\"synthetics\"][\"privacy_filters\"] = {}\n",
    "CONFIG[\"models\"][0][\"synthetics\"][\"privacy_filters\"][\"outliers\"] = None\n",
    "CONFIG[\"models\"][0][\"synthetics\"][\"privacy_filters\"][\"similarity\"] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645d2859",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we need to create some constraints for the partition strategy, this will be used to create the specific\n",
    "# partitions\n",
    "#\n",
    "# Params:\n",
    "# - header_clusters: Any header clusters desired, if omitted, we'll use all headers\n",
    "# - max_row_partitions: The max number of row \"clusters\" to use, mutually exclusive with `max_row_count`\n",
    "# - max_row_count: The max number of records to include in a row cluster\n",
    "\n",
    "constraints = strategy.PartitionConstraints(\n",
    "    header_clusters=HEADER_CLUSTERS, \n",
    "    max_row_count=MAX_ROWS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e17f0099",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create our job runner\n",
    "run = runner.StrategyRunner(\n",
    "    strategy_id=\"foo\",\n",
    "    df=DF,\n",
    "    cache_file=CACHE_FILE,\n",
    "    cache_overwrite=True,  # False means we'll try and load an existing cache and start back up, otherwise start fresh\n",
    "    model_config=CONFIG,\n",
    "    partition_constraints=constraints,\n",
    "    project=PROJECT\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a72c4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run = runner.StrategyRunner.from_completed(PROJECT, CACHE_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6931b64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "run.train_all_partitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e67bc9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "synthetic = run.get_training_synthetic_data()\n",
    "synthetic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1460ea07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run.cancel_all()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
